{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "## Please note the above functions are from MBOD Comm313 class!\n",
    "________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import calendar\n",
    "import datetime as dt\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from IPython.display import HTML\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "import re\n",
    "\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, lowercase=False, strip_chars=''):\n",
    "    '''create a list of tokens from a string by splitting on whitespace and applying optional normalization \n",
    "    \n",
    "    Args:\n",
    "        text        -- a string object containing the text to be tokenized\n",
    "        lowercase   -- should text string be normalized as lowercase (default: False)\n",
    "        strip_chars -- a string indicating characters to strip out of text, e.g. punctuation (default: empty string) \n",
    "        \n",
    "    Return:\n",
    "        A list of tokens\n",
    "    '''\n",
    "    \n",
    "    # create a replacement dictionary from the\n",
    "    # string of characters in the **strip_chars**\n",
    "    rdict = str.maketrans('','',strip_chars)\n",
    "    \n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    tokens = text.translate(rdict).split()\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#characters_to_remove = '!,.()[]\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_tokens(tokens, n=1):\n",
    "    '''create a list of n-gram tokens from a list of tokens\n",
    "    \n",
    "    Args:\n",
    "        tokens -- a list of tokens\n",
    "        n      -- the size of the window to use to build n-gram token list\n",
    "        \n",
    "    Returns:\n",
    "        \n",
    "        list of n-gram strings (whitespace separated) of length n\n",
    "    '''\n",
    "    \n",
    "    if n<2 or n>len(tokens):\n",
    "        return tokens\n",
    "    \n",
    "    new_tokens = []\n",
    "    \n",
    "    for i in range(len(tokens)-n+1):\n",
    "        new_tokens.append(\" \".join(tokens[i:i+n]))\n",
    "        \n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collocates(tokens, kw, win=[4,4]):\n",
    "    '''return the collocates in a window around a given keyword\n",
    "    \n",
    "    Args:\n",
    "          tokens -- a list of tokens\n",
    "          kw     -- keyword string to find and get collocates for\n",
    "          win    -- a list of number of tokens to left (index 0) and right (index 1) to use; default: [4,4]\n",
    "    \n",
    "    Returns:\n",
    "          a list of contexts (matching window specification) around each instance of keyword in tokens\n",
    "    '''\n",
    "    hits = [p for p,t in enumerate(tokens) if t==kw]\n",
    "    \n",
    "    context=[]\n",
    "    for hit in hits:\n",
    "        left = [] if win[0]<1 else tokens[hit-win[0]:hit]\n",
    "        right = [] if win[1]<1 else tokens[hit+1:hit+win[1]+1]\n",
    "        \n",
    "        context.extend(left)\n",
    "        context.extend(right)\n",
    "        \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_colls(texts,kw, win=[4,4]):\n",
    "    '''create a collocate frequency list for instances of a kw in a list of texts\n",
    "    \n",
    "    Args:\n",
    "        texts  -- a list of tokenized texts\n",
    "        kw     -- keyword string to find and get collocates for\n",
    "        win    -- a list of number of tokens to left (index 0) and right (index 1) to use; default: [4,4]\n",
    "    \n",
    "    Returns:\n",
    "        a list-of-tuples where each tuple is (collocate, freq_with_kw, coll_total_freq)\n",
    "    '''\n",
    "    word_dist = Counter()\n",
    "    colls = Counter()\n",
    "    for text, tokens in texts.items():\n",
    "        word_dist.update(tokens)\n",
    "        colls.update(collocates(tokens,kw, win))\n",
    "    \n",
    "    return [(str(k),v, word_dist[k]) for k,v in colls.items()], word_dist.get(kw), sum(word_dist.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_kwic(kw, text, win=4):\n",
    "    '''A basic KWIC function for a text\n",
    "    \n",
    "    Args:\n",
    "        kw   -- string match for keyword to match for each line\n",
    "        text -- a list of tokens for the text\n",
    "        \n",
    "    Return:\n",
    "        list of lines of form [ [left context words], kw, [right context words]]\n",
    "    '''\n",
    "    \n",
    "    hits = [(w,i) for i,w in enumerate(text) if w==kw]\n",
    "    \n",
    "    lines = []\n",
    "    for hit in hits:\n",
    "        left = text[hit[1]-win:hit[1]]\n",
    "        kw = text[hit[1]]\n",
    "        right = text[hit[1]+1 : hit[1]+win+1]\n",
    "        \n",
    "        \n",
    "        left = ['']*(win-len(left)) + left if len(left)<win else left\n",
    "        right = right+['']*(win-len(right)) if len(right)<win else right\n",
    "\n",
    "        \n",
    "        lines.append([left, kw, right])\n",
    "        \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_kwic(kwic, win=None):\n",
    "    '''A basic print function for a KWIC object\n",
    "    \n",
    "    Args:\n",
    "        kwic -- a list of KWIC lines of the form [ [left words], kw, [right words]]\n",
    "        win  -- if None then use all words provided in context otherwise limit by win\n",
    "        \n",
    "    Prints KWIC lines with left context width/padding win*8 characters\n",
    "    '''\n",
    "    \n",
    "    if not kwic:\n",
    "        return\n",
    "    \n",
    "    if win is None:\n",
    "        win = len(kwic[0][0])\n",
    "    \n",
    "    for line in kwic:\n",
    "        print(\"{: >{}}  {}  {}\".format(' '.join(line[0][-win:]), \n",
    "                                      win*10, \n",
    "                                      line[1], \n",
    "                                      ' '.join(line[2][:win])\n",
    "                                     )\n",
    "             )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_kwic(kwic, order=None):\n",
    "    ''' sort a kwic list using the passed positional arguments \n",
    "    \n",
    "    Args:\n",
    "        kwic   -- a list of lists [ [left tokens], kw, [right tokens]]\n",
    "        order  -- a list of one or more positional arguments of form side-pos, e.g. L1, R3, L4 (default: None)\n",
    "    \n",
    "    Returns:\n",
    "        kwic sorted for each positional argument in reverse, i.e. ['R1','L1'] sorts first by L1 and then R1\n",
    "    '''\n",
    "    if order is None:\n",
    "        return kwic\n",
    "   \n",
    "    order = [order] if not type(order) is list else order\n",
    "    order.reverse()\n",
    "    \n",
    "    for sort_term in order:\n",
    "        if not re.match('[LR][1-4]', sort_term):\n",
    "            pass\n",
    "        \n",
    "        pos1 = 0 if sort_term[0]=='L' else 2\n",
    "        pos2 = int(sort_term[1])-1\n",
    "        pos2 = 3-pos2 if sort_term[0]=='L' else pos2\n",
    "        kwic.sort(key=lambda l : l[pos1][pos2])\n",
    "    \n",
    "    return kwic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________\n",
    "# Custom Function for Dr Turow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JTurow_POS_Extract(directory_with_txt_file):\n",
    "#######\n",
    "\n",
    "    '''\n",
    "    Etienne Jacquot - ASC IT SYSADMIN\n",
    "    09/05/19\n",
    "    \n",
    "    Function for Dr Turow, extracting NLP data from Factivia articles related to Smart Devices:\n",
    "    This is a draft work-in-progress, as in not entirely robust in terms of entering desired POS tags...\n",
    "\n",
    "    Input - directory_with_txt_file: \n",
    "        This should be a string value, the directory with article files in question. \n",
    "        Make sure this file is converted from .rtf Factivia download to .txt utf-16\n",
    "            \n",
    "    Output - total_most_common_nltk_NVJ: \n",
    "        This is a Counter() object which contains most common tokens\n",
    "        Nouns, Verbs, and Adjectives found in articles contained in the text file\n",
    "    '''\n",
    "\n",
    "\n",
    "    # Read file of text data\n",
    "    for item in os.listdir(directory_with_txt_file):\n",
    "        if item.endswith('.txt'):\n",
    "            txt_file = item\n",
    "            print('Opening the following text file:', item, '\\n')\n",
    "            text = open(directory_with_txt_file + item,'r',encoding=\"UTF-16\").read().splitlines()\n",
    "\n",
    "    # Preparing to capture data as list of dictionaries\n",
    "    articles_total=[]\n",
    "    articles = {}\n",
    "    article_txt=[]\n",
    "\n",
    "    print('Extracting data...\\n')\n",
    "    # Go through lines in article to separate individual articles\n",
    "    for line in text:\n",
    "        article_txt += [line]\n",
    "        # Each article ends with this Document Line\n",
    "        if line.startswith('Document ') and len(line) == 34:\n",
    "            doc_id = line.split()[1]\n",
    "            articles = {'document_ID':doc_id,\n",
    "                        'article_text':article_txt, \n",
    "                        'tokens':[],\n",
    "                        'POS_tag':[]\n",
    "                            }\n",
    "            articles_total.append(articles)    \n",
    "            article_txt = []\n",
    "    print('Total number of articles in text file:', len(articles_total),'\\n')\n",
    "\n",
    "    # Extracting tokens from article texts and updating the dictionary\n",
    "    print('Tokenizing text in each article (lowercase & removing the following chars:', characters_to_remove,')\\n')\n",
    "    tokens = []\n",
    "    total_tokens = []\n",
    "    for article in articles_total:\n",
    "        for words in article['article_text']:\n",
    "            tokens += tokenize(words,strip_chars=characters_to_remove,lowercase=True)\n",
    "        # Also creating total_tokens which is all tokens in one list\n",
    "        total_tokens += tokens\n",
    "        article['tokens']+= tokens\n",
    "        tokens = []\n",
    "\n",
    "    # Applying POS tagging\n",
    "    print('Applying POS tags on tokens, please be patient...\\n')\n",
    "    for article in articles_total:\n",
    "        for words in article['tokens']:\n",
    "            #print(words,'... now working on NLTK:')\n",
    "            word = [words]\n",
    "            #print('word is:',word)\n",
    "            nltk_text = nltk.pos_tag(word)\n",
    "            #print('nltk is:',nltk_text,'\\n')\n",
    "            article['POS_tag'] += nltk_text\n",
    "\n",
    "    # Identifying words that match target POS tag\n",
    "    print('Extracting most frequent tokens with POS Tags: NN*, V*, and JJ*','\\n')\n",
    "    total_nltk_tokens = []\n",
    "    target_nltk_tokens = []\n",
    "    IN_and_PRP_tokens = []\n",
    "    NNJJV_nlkt_tokens = []\n",
    "\n",
    "    for article in articles_total:\n",
    "        for words in article['POS_tag']:\n",
    "            total_nltk_tokens += [words]\n",
    "\n",
    "            # Excluding Prepositions & Pronouns as requested in original ticket by Dr Turow\n",
    "            # targeted_ = (words[1] == 'IN' or words[1] == 'FW')\n",
    "            #if not targeted_:\n",
    "                #target_nltk_tokens += [words]\n",
    "\n",
    "            # Including Nouns, Verbs, Adjectives\n",
    "            targeted_nouns_verbs_adj = (words[1].startswith('NN') or words[1].startswith('JJ') or words[1].startswith('V'))\n",
    "            if targeted_nouns_verbs_adj:\n",
    "                NNJJV_nlkt_tokens += [words]\n",
    "\n",
    "            else:\n",
    "                IN_and_PRP_tokens += [words]\n",
    "\n",
    "    if (len(total_nltk_tokens) - len(target_nltk_tokens)) or (len(total_nltk_tokens) - len(NNJJV_nlkt_tokens)) == len(IN_and_PRP_tokens):\n",
    "        print('Successfully extracted all words that match target POS tagging!\\n')\n",
    "\n",
    "    # Counting most common words that match targetted POS\n",
    "    print('Counting most frequent Nouns, Verbs, & Adjectives in file:\\n')\n",
    "    total_most_common_nltk_NVJ = Counter()\n",
    "    for tokens in NNJJV_nlkt_tokens:\n",
    "        total_most_common_nltk_NVJ.update(get_ngram_tokens([tokens],1))\n",
    "    #print(total_most_common_nltk_NVJ.most_common(30))\n",
    "    print('Completed!\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
